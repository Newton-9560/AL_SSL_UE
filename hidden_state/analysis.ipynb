{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_generated_result\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file:  eef4e351-eca5-4221-9c9e-f647be914de2.npy\n"
     ]
    }
   ],
   "source": [
    "result = load_generated_result('opt', 'truthful_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sar auroc: 0.6124151192836969\n",
      "maximumsequenceprobability auroc: 0.5053091761312422\n",
      "semanticentropy auroc: 0.47413016044708844\n",
      "lexicalsimilarity auroc: 0.5826693107385373\n",
      "montecarlosequenceentropy auroc: 0.44771347875728623\n"
     ]
    }
   ],
   "source": [
    "us_metrics = ['sar', 'maximumsequenceprobability', 'semanticentropy', 'lexicalsimilarity', 'montecarlosequenceentropy']\n",
    "correctness = [data['align']>0.5 for data in result]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for metric in us_metrics:\n",
    "    auroc = roc_auc_score(correctness, -np.array([data[metric] for data in result]))\n",
    "    print(f'{metric} auroc: {auroc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels_by_uncertainty(uncertainty_values, true_labels, high_threshold=0.3, low_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Assigns labels based on uncertainty values.\n",
    "    - The top `high_threshold` proportion of uncertainty values are assigned True (1).\n",
    "    - The bottom `low_threshold` proportion of uncertainty values are assigned False (0).\n",
    "    \n",
    "    Parameters:\n",
    "        uncertainty_values (list or np.array): The uncertainty scores.\n",
    "        true_labels (list or np.array): The ground truth labels.\n",
    "        high_threshold (float): The proportion of highest uncertainty values to assign True.\n",
    "        low_threshold (float): The proportion of lowest uncertainty values to assign False.\n",
    "        \n",
    "    Returns:\n",
    "        assigned_labels (np.array): The assigned labels (-1 for unassigned).\n",
    "        accuracy (float): The accuracy of assigned labels compared to true labels.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    uncertainty_values = np.array(uncertainty_values)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Determine threshold indices\n",
    "    num_high = int(len(uncertainty_values) * high_threshold)\n",
    "    num_low = int(len(uncertainty_values) * low_threshold)\n",
    "\n",
    "    # Get sorted indices in ascending order\n",
    "    sorted_indices = np.argsort(uncertainty_values)\n",
    "\n",
    "    # Assign labels\n",
    "    assigned_labels = np.full_like(true_labels, -1)  # Initialize with -1 (unassigned)\n",
    "    assigned_labels[sorted_indices[:num_low]] = 0   # Lowest 40% assigned False (0)\n",
    "    assigned_labels[sorted_indices[-num_high:]] = 1 # Highest 30% assigned True (1)\n",
    "\n",
    "    # Compute accuracy (only on assigned labels)\n",
    "    mask = assigned_labels != -1  # Consider only assigned labels\n",
    "    accuracy = np.mean(assigned_labels[mask] == true_labels[mask])\n",
    "\n",
    "    return assigned_labels, accuracy\n",
    "\n",
    "def assign_labels_by_uncertainty_2(uncertainty1, uncertainty2, true_labels, high_threshold=0.3, low_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Assigns labels based on two uncertainty values.\n",
    "    - The samples that are in the top `high_threshold` proportion in **both** uncertainty values are assigned True (1).\n",
    "    - The samples that are in the bottom `low_threshold` proportion in **both** uncertainty values are assigned False (0).\n",
    "    - Middle values remain unassigned (-1).\n",
    "    \n",
    "    Parameters:\n",
    "        uncertainty1 (list or np.array): The first set of uncertainty scores.\n",
    "        uncertainty2 (list or np.array): The second set of uncertainty scores.\n",
    "        true_labels (list or np.array): The ground truth labels.\n",
    "        high_threshold (float): The proportion of highest uncertainty values to assign True.\n",
    "        low_threshold (float): The proportion of lowest uncertainty values to assign False.\n",
    "        \n",
    "    Returns:\n",
    "        assigned_labels (np.array): The assigned labels (-1 for unassigned).\n",
    "        accuracy (float): The accuracy of assigned labels compared to true labels.\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    uncertainty1 = np.array(uncertainty1)\n",
    "    uncertainty2 = np.array(uncertainty2)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Determine threshold indices\n",
    "    num_high = int(len(uncertainty1) * high_threshold)\n",
    "    num_low = int(len(uncertainty1) * low_threshold)\n",
    "\n",
    "    # Get sorted indices\n",
    "    sorted_indices1 = np.argsort(uncertainty1)  # Ascending order for first uncertainty\n",
    "    sorted_indices2 = np.argsort(uncertainty2)  # Ascending order for second uncertainty\n",
    "\n",
    "    # Identify top and bottom values\n",
    "    high_set = set(sorted_indices1[-num_high:]) & set(sorted_indices2[-num_high:])  # Top 30% in both\n",
    "    low_set = set(sorted_indices1[:num_low]) & set(sorted_indices2[:num_low])  # Bottom 40% in both\n",
    "\n",
    "    # Assign labels\n",
    "    assigned_labels = np.full_like(true_labels, -1)  # Initialize with -1 (unassigned)\n",
    "    assigned_labels[list(low_set)] = 0   # Assign False (0)\n",
    "    assigned_labels[list(high_set)] = 1  # Assign True (1)\n",
    "\n",
    "    # Compute accuracy (only for assigned labels)\n",
    "    mask = assigned_labels != -1  # Consider only assigned labels\n",
    "    accuracy = np.mean(assigned_labels[mask] == true_labels[mask]) if np.any(mask) else 0.0\n",
    "\n",
    "    return assigned_labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True, False, ...,  True,  True, False]), 0.83275)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([i['align']>0.7 for i in result])\n",
    "uncertainty1 = np.array([-i['sar'] for i in result])\n",
    "uncertainty2 = np.array([-i['maximumsequenceprobability'] for i in result])\n",
    "# assign_labels_by_uncertainty_2(uncertainty1, uncertainty2, labels, high_threshold=0.05, low_threshold=0.05)\n",
    "assign_labels_by_uncertainty(uncertainty1, labels, high_threshold=0.8, low_threshold=0.54)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
