{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from hidden_state.utils import load_generated_result\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import calculate_auroc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidden_state.utils import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file:  8aa76b7b-1009-4ac4-8d7f-fc2f73ce5030.npy\n",
      "Reading file:  a5c457d5-d186-43ad-88db-ccfced0c8b1a.npy\n",
      "Reading file:  36ad2a1e-57aa-409d-a340-2d008411aaf5.npy\n",
      "Reading file:  fb13648e-a9b6-42e8-a623-36ec4d4bf68f.npy\n",
      "Reading file:  065d22ef-6d36-4bf9-9839-99a4310ffdd0.npy\n",
      "Reading file:  7dbdb047-9af1-4c2c-bb99-f58bc86072f3.npy\n",
      "Reading file:  5466e6ef-b16b-4a72-8938-7d7d0fd62c93.npy\n",
      "Reading file:  d78b31ac-5ddb-4f72-a77c-9f786c2cbbea.npy\n",
      "Reading file:  13bdce48-90ab-4bf5-bfd5-c94fa2fd8ee5.npy\n",
      "Reading file:  491a9947-fe95-44d2-9aa5-8f709ea26ff4.npy\n",
      "Reading file:  be0bfcdb-1253-4d01-a94d-bca15a97cf82.npy\n",
      "Reading file:  6a47c36c-9368-4dc7-b7f9-e9221f23c351.npy\n",
      "Reading file:  ed5ccec2-335d-4024-af25-4f5cf57d12af.npy\n",
      "Reading file:  46f0784e-25e7-4f38-9705-3cdc67ff16be.npy\n",
      "Reading file:  c5648dc5-8b99-4dd3-a492-f459caf41e4f.npy\n",
      "Reading file:  592f3f30-358f-4893-b8e9-49cc2bbb43b5.npy\n",
      "Reading file:  5098d0bd-cfc9-4765-94ab-6430323954ad.npy\n",
      "Reading file:  70249c18-dd72-4e65-9365-db3b9b88f50c.npy\n",
      "Reading file:  10da7d99-9014-4123-8676-828a70cfdd72.npy\n",
      "Reading file:  200b310d-855f-4a29-80a0-7729b86c25bd.npy\n",
      "Reading file:  2eee8fc5-8339-4970-a713-fcac61c037b6.npy\n",
      "Reading file:  e61e6dfc-40fd-4af6-897c-a323b0080be9.npy\n",
      "Reading file:  815fac20-35d6-4941-b5d3-bc7496c4fe00.npy\n",
      "Reading file:  077da3b7-7c65-45a8-b709-848988c9e09a.npy\n",
      "Reading file:  091483e8-045b-4366-a5d4-4aad7d315f56.npy\n",
      "Reading file:  23764e90-fdf0-4f00-b4b1-0383529fb152.npy\n",
      "Reading file:  bb873fa0-30f3-4a47-a0c6-862a46c560f5.npy\n",
      "Reading file:  1c12337b-fc62-4ccb-8417-dd53df0fa5c5.npy\n",
      "Reading file:  105730ac-6507-4253-80d2-6a95dcd61667.npy\n",
      "Reading file:  d9ad4d34-a722-43a4-afe2-ebce2660c9d4.npy\n",
      "Reading file:  eef4e351-eca5-4221-9c9e-f647be914de2.npy\n",
      "Reading file:  93d4b0ca-6a28-4248-b599-c8eefcbfd983.npy\n",
      "metric                                lexicalsimilarity  \\\n",
      "model                    dataset                          \n",
      "Mistral-7B-Instruct-v0.3 coqa                     71.50   \n",
      "                         simple_qa                55.15   \n",
      "                         trivia_qa                75.67   \n",
      "                         truthful_qa              64.22   \n",
      "llama3                   ambig_qa                 75.89   \n",
      "                         coqa                     73.11   \n",
      "                         sciq                     75.52   \n",
      "                         simple_qa                58.88   \n",
      "                         squad                    81.90   \n",
      "                         trivia_qa                85.28   \n",
      "                         truthful_qa              64.52   \n",
      "                         tydiqa                   78.79   \n",
      "opt                      ambig_qa                 64.29   \n",
      "                         coqa                     70.01   \n",
      "                         sciq                     64.42   \n",
      "                         trivia_qa                71.77   \n",
      "                         truthful_qa              57.41   \n",
      "qwen                     ambig_qa                 74.86   \n",
      "                         coqa                     71.02   \n",
      "                         sciq                     69.03   \n",
      "                         simple_qa                56.75   \n",
      "                         squad                    76.05   \n",
      "                         trivia_qa                82.47   \n",
      "                         truthful_qa              65.14   \n",
      "                         tydiqa                   71.31   \n",
      "qwen14b                  ambig_qa                 72.40   \n",
      "                         sciq                     46.90   \n",
      "                         simple_qa                56.35   \n",
      "                         squad                    72.96   \n",
      "                         trivia_qa                80.23   \n",
      "                         truthful_qa              71.07   \n",
      "                         tydiqa                   69.21   \n",
      "\n",
      "metric                                maximumsequenceprobability  \\\n",
      "model                    dataset                                   \n",
      "Mistral-7B-Instruct-v0.3 coqa                              70.33   \n",
      "                         simple_qa                         56.46   \n",
      "                         trivia_qa                         70.02   \n",
      "                         truthful_qa                       53.64   \n",
      "llama3                   ambig_qa                          70.60   \n",
      "                         coqa                              72.02   \n",
      "                         sciq                              73.25   \n",
      "                         simple_qa                         57.46   \n",
      "                         squad                             77.77   \n",
      "                         trivia_qa                         83.43   \n",
      "                         truthful_qa                       59.06   \n",
      "                         tydiqa                            77.80   \n",
      "opt                      ambig_qa                          85.71   \n",
      "                         coqa                              72.60   \n",
      "                         sciq                              71.49   \n",
      "                         trivia_qa                         78.21   \n",
      "                         truthful_qa                       50.23   \n",
      "qwen                     ambig_qa                          74.33   \n",
      "                         coqa                              69.91   \n",
      "                         sciq                              66.32   \n",
      "                         simple_qa                         58.96   \n",
      "                         squad                             73.45   \n",
      "                         trivia_qa                         81.73   \n",
      "                         truthful_qa                       62.14   \n",
      "                         tydiqa                            72.07   \n",
      "qwen14b                  ambig_qa                          70.51   \n",
      "                         sciq                              47.78   \n",
      "                         simple_qa                         52.92   \n",
      "                         squad                             71.79   \n",
      "                         trivia_qa                         78.18   \n",
      "                         truthful_qa                       67.11   \n",
      "                         tydiqa                            70.70   \n",
      "\n",
      "metric                                montecarlosequenceentropy    sar  \\\n",
      "model                    dataset                                         \n",
      "Mistral-7B-Instruct-v0.3 coqa                             72.36  72.20   \n",
      "                         simple_qa                        56.90  56.32   \n",
      "                         trivia_qa                        69.12  80.19   \n",
      "                         truthful_qa                      53.65  67.32   \n",
      "llama3                   ambig_qa                         70.79  78.65   \n",
      "                         coqa                             73.29  73.69   \n",
      "                         sciq                             74.00  76.34   \n",
      "                         simple_qa                        64.25  62.86   \n",
      "                         squad                            80.72  83.13   \n",
      "                         trivia_qa                        83.43  86.95   \n",
      "                         truthful_qa                      61.75  67.21   \n",
      "                         tydiqa                           77.54  79.07   \n",
      "opt                      ambig_qa                         71.43  64.29   \n",
      "                         coqa                             61.23  72.55   \n",
      "                         sciq                             60.27  65.83   \n",
      "                         trivia_qa                        62.04  73.82   \n",
      "                         truthful_qa                      46.44  60.27   \n",
      "qwen                     ambig_qa                         75.95  78.83   \n",
      "                         coqa                             71.60  72.92   \n",
      "                         sciq                             68.06  72.26   \n",
      "                         simple_qa                        64.95  64.82   \n",
      "                         squad                            74.79  77.65   \n",
      "                         trivia_qa                        81.71  84.44   \n",
      "                         truthful_qa                      66.43  64.77   \n",
      "                         tydiqa                           72.53  73.11   \n",
      "qwen14b                  ambig_qa                         70.51  74.89   \n",
      "                         sciq                             51.69  55.87   \n",
      "                         simple_qa                        54.91  58.43   \n",
      "                         squad                            72.76  72.82   \n",
      "                         trivia_qa                        77.79  83.27   \n",
      "                         truthful_qa                      68.83  71.38   \n",
      "                         tydiqa                           70.81  69.90   \n",
      "\n",
      "metric                                semanticentropy  \n",
      "model                    dataset                       \n",
      "Mistral-7B-Instruct-v0.3 coqa                   72.49  \n",
      "                         simple_qa              57.50  \n",
      "                         trivia_qa              70.87  \n",
      "                         truthful_qa            57.58  \n",
      "llama3                   ambig_qa               72.28  \n",
      "                         coqa                   73.41  \n",
      "                         sciq                   75.16  \n",
      "                         simple_qa              61.86  \n",
      "                         squad                  81.13  \n",
      "                         trivia_qa              83.87  \n",
      "                         truthful_qa            61.48  \n",
      "                         tydiqa                 78.33  \n",
      "opt                      ambig_qa               50.00  \n",
      "                         coqa                   61.32  \n",
      "                         sciq                   59.24  \n",
      "                         trivia_qa              63.52  \n",
      "                         truthful_qa            47.89  \n",
      "qwen                     ambig_qa               76.45  \n",
      "                         coqa                   72.36  \n",
      "                         sciq                   68.64  \n",
      "                         simple_qa              63.32  \n",
      "                         squad                  75.46  \n",
      "                         trivia_qa              82.61  \n",
      "                         truthful_qa            63.90  \n",
      "                         tydiqa                 72.61  \n",
      "qwen14b                  ambig_qa               72.04  \n",
      "                         sciq                   46.11  \n",
      "                         simple_qa              54.25  \n",
      "                         squad                  71.94  \n",
      "                         trivia_qa              79.61  \n",
      "                         truthful_qa            67.89  \n",
      "                         tydiqa                 71.61  \n"
     ]
    }
   ],
   "source": [
    "us_metrics = ['sar', 'maximumsequenceprobability', 'semanticentropy', 'lexicalsimilarity', 'montecarlosequenceentropy']\n",
    "records = []\n",
    "\n",
    "for model_name in Path('./data').iterdir():\n",
    "    for dataset_name in model_name.iterdir():\n",
    "        result = load_generated_result(model_name.name, dataset_name.name)\n",
    "        for metric in us_metrics:\n",
    "            auroc = calculate_auroc(result, metric, threshold=0.5)\n",
    "            records.append({\n",
    "                'model': model_name.name,\n",
    "                'dataset': dataset_name.name,\n",
    "                'metric': metric,\n",
    "                'auroc': auroc\n",
    "            })\n",
    "\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df = result_df.pivot_table(index=['model', 'dataset'], columns='metric', values='auroc')\n",
    "result_df = result_df * 100\n",
    "result_df = result_df.round(2)\n",
    "# result_df.to_csv('/home/hanwenli/work/2025/AL_SSL/results/experiments/ue_auroc_threshold_7.csv')\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
