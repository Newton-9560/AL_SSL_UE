{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/hanwenli/work/2025/AL_SSL/results_with_num_unlabeled_llama3.csv')\n",
    "trivia_qa_df = dataset[dataset['dataset'] == 'trivia_qa']\n",
    "trivia_qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    {'name':'SAR','value':trivia_qa_df[trivia_qa_df['uncertainty_type']=='sar']['auroc'].to_list()[:-7], 'num_unlabeled':trivia_qa_df[trivia_qa_df['uncertainty_type']=='sar']['unlabeled_size'].to_list()[:-7]},\n",
    "    {'name':'SE','value':trivia_qa_df[trivia_qa_df['uncertainty_type']=='semanticentropy']['auroc'].to_list()[:-7], 'num_unlabeled':trivia_qa_df[trivia_qa_df['uncertainty_type']=='semanticentropy']['unlabeled_size'].to_list()[:-7]},\n",
    "    {'name':'MSP','value':trivia_qa_df[trivia_qa_df['uncertainty_type']=='maximumsequenceprobability']['auroc'].to_list()[:-7], 'num_unlabeled':trivia_qa_df[trivia_qa_df['uncertainty_type']=='maximumsequenceprobability']['unlabeled_size'].to_list()[:-7]},\n",
    "    {'name':'LS','value':trivia_qa_df[trivia_qa_df['uncertainty_type']=='lexicalsimilarity']['auroc'].to_list()[:-7], 'num_unlabeled':trivia_qa_df[trivia_qa_df['uncertainty_type']=='lexicalsimilarity']['unlabeled_size'].to_list()[:-7]},\n",
    "    {'name':'MCSE','value':trivia_qa_df[trivia_qa_df['uncertainty_type']=='montecarlosequenceentropy']['auroc'].to_list()[:-7], 'num_unlabeled':trivia_qa_df[trivia_qa_df['uncertainty_type']=='montecarlosequenceentropy']['unlabeled_size'].to_list()[:-7]}\n",
    "]\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty_types(data_list, output_filename='uncertainty_auroc_plot.png'):\n",
    "    \"\"\"\n",
    "    Plot AUROC values for different uncertainty types with unlabeled size as x-axis\n",
    "    \n",
    "    Parameters:\n",
    "    data_list: list of dictionaries with 'name', 'value', and 'num_unlabeled' keys\n",
    "    output_filename: str, filename to save the plot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    colors = sns.color_palette('colorblind', len(data_list))\n",
    "\n",
    "    \n",
    "    # Define colors and markers for each uncertainty type\n",
    "    style_settings = {\n",
    "        'SAR': {'color': '#1b263b', 'marker': 'o'},\n",
    "        'SE': {'color': '#3a6ea5', 'marker': 'o'},\n",
    "        'MSP': {'color': '#778da9', 'marker': 'o'},\n",
    "        'LS': {'color': '#C4C4C4', 'marker': 'o'},\n",
    "        'MCSE': {'color': '#bf0000', 'marker': 'o'}\n",
    "    }\n",
    "    \n",
    "    # Create a figure with sufficient size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot each uncertainty type\n",
    "    for i, item in enumerate(data_list):\n",
    "        name = item['name']\n",
    "        values = item['value']\n",
    "        num_unlabeled = item['num_unlabeled']\n",
    "        \n",
    "        # Get style settings\n",
    "        style = style_settings.get(name, {'color': 'black', 'marker': 'x'})\n",
    "        \n",
    "        # Plot the line\n",
    "        plt.plot(\n",
    "            num_unlabeled,\n",
    "            values,\n",
    "            label=name,\n",
    "            color=colors[i],\n",
    "            marker=style['marker'],\n",
    "            linestyle='-',\n",
    "            linewidth=2\n",
    "        )\n",
    "    \n",
    "    # Set the x-axis and y-axis labels\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    # Add grid for better readability\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(fontsize=20, loc='lower right')\n",
    "    \n",
    "    # Adjust layout to make sure everything fits\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Display the chart\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your data\n",
    "plot_uncertainty_types(data_list, 'uncertainty_auroc_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.int8)\n",
    "model.config.pad_token_id = 128001\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "prompt = \"Answer the following question concisely: What is the capital of France?\\n Answer: \"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids, max_new_tokens=inputs.input_ids.shape[1]+5, attention_mask=attention_mask)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_new_tokens=inputs.input_ids.shape[1]+5, attention_mask=attention_mask)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanwenli/anaconda3/envs/debug/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:40<00:00,  1.36s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question concisely: What is the capital of France?\n",
      " Answer:  Paris\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "input_text = \"Answer the following question concisely: What is the capital of France?\\n Answer: \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(**input_ids, max_new_tokens=10)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "Answer the question concisely.Q: What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "simple_qa = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\" + \"Answer the question concisely.\" + \"Q: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "simple_qa = simple_qa.format(question=\"What is the capital of France?\")\n",
    "print(simple_qa)\n",
    "input_ids = tokenizer(simple_qa, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user\\nAnswer the question concisely.Q: What is the capital of France?assistant\\n\\nParis']\n"
     ]
    }
   ],
   "source": [
    "generate_ids = model.generate(input_ids.input_ids, max_new_tokens=10, attention_mask=input_ids.attention_mask)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
